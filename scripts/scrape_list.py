#!/usr/bin/env python3
"""
æŠ“å–å…¬è€ƒé›·è¾¾èŒä½åˆ—è¡¨

æ”¯æŒåˆ†é¡µæŠ“å–

ä½¿ç”¨æ–¹æ³•:
    python scripts/scrape_list.py [--pages N]
    
ç¤ºä¾‹:
    python scripts/scrape_list.py --pages 10
"""

import argparse
import asyncio
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path

# é…ç½®
BASE_URL = "https://www.gongkaoleida.com"
LIST_URL_TEMPLATE = BASE_URL + "/area/878-0-0-0-124?page={page}"
DATA_DIR = Path(__file__).parent.parent / "data"

# ç­›é€‰è§„åˆ™
EXCLUDE_KEYWORDS = ["æˆç»©", "åå•", "é¢è¯•", "ä½“æ£€", "é¢†å–", "èµ„æ ¼å®¡æŸ¥", "å…¬ç¤º", "å½•ç”¨", "é€šçŸ¥"]
INCLUDE_KEYWORDS = ["æ‹›è˜", "æ‹›å‹Ÿ", "é€‰è˜", "æ‹›è€ƒ", "é´é€‰", "é€‰è°ƒ"]


def is_recruitment_post(title: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ‹›è˜å…¬å‘Š"""
    for keyword in EXCLUDE_KEYWORDS:
        if keyword in title:
            return False
    for keyword in INCLUDE_KEYWORDS:
        if keyword in title:
            return True
    return False


async def fetch_page(page_num: int, playwright) -> list:
    """æŠ“å–å•é¡µèŒä½åˆ—è¡¨"""
    url = LIST_URL_TEMPLATE.format(page=page_num)
    jobs = []
    
    browser = await playwright.chromium.launch(headless=True)
    context = await browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    )
    page = await context.new_page()
    
    try:
        print(f"   ğŸ“„ åŠ è½½ç¬¬ {page_num} é¡µ...")
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
        await asyncio.sleep(3)
        
        # è·å–æ‰€æœ‰èŒä½é“¾æ¥
        items = await page.evaluate("""
            () => {
                const results = [];
                const links = document.querySelectorAll('a');
                
                links.forEach(link => {
                    const href = link.href;
                    const title = link.innerText.trim();
                    
                    if (href && title && title.length > 10 && 
                        (href.includes('/article/') || href.includes('/info/'))) {
                        results.push({
                            title: title.substring(0, 200),
                            url: href
                        });
                    }
                });
                
                return results;
            }
        """)
        
        for item in items:
            title = item.get("title", "")
            full_url = item.get("url", "")
            
            if not is_recruitment_post(title):
                continue
            
            jobs.append({
                "title": title,
                "url": full_url,
                "date": "",  # æ—¥æœŸå°†ä»è¯¦æƒ…é¡µæå–
                "source": ""
            })
        
        print(f"      æ‰¾åˆ° {len(jobs)} æ¡æ‹›è˜å…¬å‘Š")
        
    except Exception as e:
        print(f"   âš ï¸ ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥: {e}")
    finally:
        await browser.close()
    
    return jobs


async def fetch_list(max_pages: int) -> list:
    """ä½¿ç”¨ Playwright æŠ“å–èŒä½åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ è¯·å®‰è£… playwright")
        sys.exit(1)
    
    all_jobs = []
    print(f"ğŸ“‹ å¼€å§‹æŠ“å–æ‹›è˜ä¿¡æ¯...")
    print(f"ğŸ”¢ æœ€å¤§é¡µæ•°: {max_pages}")
    
    async with async_playwright() as p:
        empty_pages = 0
        for page_num in range(1, max_pages + 1):
            page_jobs = await fetch_page(page_num, p)
            
            if not page_jobs:
                empty_pages += 1
                if empty_pages >= 2:
                    print(f"   è¿ç»­ {empty_pages} é¡µæ— å†…å®¹ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            else:
                empty_pages = 0
                all_jobs.extend(page_jobs)
            
            await asyncio.sleep(1)
    
    # å»é‡
    seen = set()
    unique_jobs = []
    for job in all_jobs:
        if job["url"] not in seen:
            seen.add(job["url"])
            unique_jobs.append(job)
    
    print(f"âœ… å…±æ‰¾åˆ° {len(unique_jobs)} æ¡æ‹›è˜å…¬å‘Š")
    return unique_jobs


def main():
    parser = argparse.ArgumentParser(description="æŠ“å–å…¬è€ƒé›·è¾¾èŒä½åˆ—è¡¨")
    parser.add_argument("--pages", help="æœ€å¤§é¡µæ•°", type=int,
                        default=int(os.environ.get("MAX_PAGES", "5")))
    args = parser.parse_args()
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    jobs = asyncio.run(fetch_list(args.pages))
    
    if not jobs:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ‹›è˜å…¬å‘Š")
        return
    
    # ä¿å­˜
    today_str = datetime.now().strftime("%Y%m%d")
    output_file = DATA_DIR / f"job_list_{today_str}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š å…± {len(jobs)} æ¡èŒä½")
    for i, job in enumerate(jobs[:5], 1):
        print(f"   {i}. {job['title'][:50]}...")


if __name__ == "__main__":
    main()
