#!/usr/bin/env python3
"""
æŠ“å–å…¬è€ƒé›·è¾¾èŒä½åˆ—è¡¨

æ”¯æŒåˆ†é¡µå’Œæ—¥æœŸç­›é€‰

ä½¿ç”¨æ–¹æ³•:
    python scripts/scrape_list.py [--date YYYY-MM] [--pages N]
    
ç¤ºä¾‹:
    python scripts/scrape_list.py --date 2025-12 --pages 10
"""

import argparse
import asyncio
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path

# é…ç½®
BASE_URL = "https://www.gongkaoleida.com"
LIST_URL_TEMPLATE = BASE_URL + "/area/878-0-0-0-124?page={page}"
DATA_DIR = Path(__file__).parent.parent / "data"

# ç­›é€‰è§„åˆ™
EXCLUDE_KEYWORDS = ["æˆç»©", "åå•", "é¢è¯•", "ä½“æ£€", "é¢†å–", "èµ„æ ¼å®¡æŸ¥", "å…¬ç¤º", "å½•ç”¨", "é€šçŸ¥"]
INCLUDE_KEYWORDS = ["æ‹›è˜", "æ‹›å‹Ÿ", "é€‰è˜", "æ‹›è€ƒ", "é´é€‰", "é€‰è°ƒ"]


def is_recruitment_post(title: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ‹›è˜å…¬å‘Š"""
    for keyword in EXCLUDE_KEYWORDS:
        if keyword in title:
            return False
    for keyword in INCLUDE_KEYWORDS:
        if keyword in title:
            return True
    return False


def matches_date(text: str, target_yearmonth: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç›®æ ‡å¹´æœˆ"""
    # target_yearmonth æ ¼å¼: "2025-12"
    if not target_yearmonth:
        return True
    
    year, month = target_yearmonth.split("-")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´æœˆ
    patterns = [
        f"{year}å¹´{int(month)}æœˆ",
        f"{year}-{month}",
        f"{year}-{int(month):02d}",
        f"{year}/{month}",
    ]
    
    for pattern in patterns:
        if pattern in text:
            return True
    
    return False


async def fetch_page(page, playwright, target_date: str = None) -> list:
    """æŠ“å–å•é¡µèŒä½åˆ—è¡¨"""
    url = LIST_URL_TEMPLATE.format(page=page)
    jobs = []
    
    browser = await playwright.chromium.launch(headless=True)
    context = await browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    )
    page_obj = await context.new_page()
    
    try:
        print(f"   ğŸ“„ åŠ è½½ç¬¬ {page} é¡µ...")
        await page_obj.goto(url, timeout=60000, wait_until="domcontentloaded")
        await asyncio.sleep(3)
        
        # è·å–æ‰€æœ‰é“¾æ¥å’Œå‘å¸ƒæ—¶é—´
        items = await page_obj.evaluate("""
            () => {
                const results = [];
                const links = document.querySelectorAll('a');
                
                links.forEach(link => {
                    const href = link.href;
                    const title = link.innerText.trim();
                    
                    // å°è¯•è·å–æ—¥æœŸä¿¡æ¯ï¼ˆä»é™„è¿‘å…ƒç´ ï¼‰
                    let dateText = '';
                    const parent = link.parentElement;
                    if (parent) {
                        const dateEl = parent.querySelector('.date, .time, [class*="date"], [class*="time"]');
                        if (dateEl) dateText = dateEl.innerText;
                    }
                    
                    // è·å–æ‰€æœ‰æ–‡æœ¬ç”¨äºæ—¥æœŸåŒ¹é…
                    const fullText = link.closest('li, .item, [class*="item"]')?.innerText || '';
                    
                    if (href && title && title.length > 10 && (href.includes('/article/') || href.includes('/info/'))) {
                        results.push({
                            title: title.substring(0, 200),
                            url: href,
                            dateText: dateText,
                            context: fullText.substring(0, 500)
                        });
                    }
                });
                
                return results;
            }
        """)
        
        for item in items:
            title = item.get("title", "")
            full_url = item.get("url", "")
            context_text = item.get("context", "") + " " + title
            
            # æ—¥æœŸç­›é€‰
            if target_date and not matches_date(context_text, target_date):
                continue
            
            if not is_recruitment_post(title):
                continue
            
            jobs.append({
                "title": title,
                "url": full_url,
                "date": target_date or datetime.now().strftime("%Y-%m-%d"),
                "source": "",
                "date_text": item.get("dateText", "")
            })
        
    except Exception as e:
        print(f"   âš ï¸ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥: {e}")
    finally:
        await browser.close()
    
    return jobs


async def fetch_list(target_date: str, max_pages: int) -> list:
    """ä½¿ç”¨ Playwright æŠ“å–èŒä½åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ è¯·å®‰è£… playwright: pip install playwright && playwright install chromium")
        sys.exit(1)
    
    all_jobs = []
    print(f"ğŸ“‹ å¼€å§‹æŠ“å– {target_date or 'æœ€æ–°'} çš„æ‹›è˜ä¿¡æ¯...")
    print(f"ğŸ”¢ æœ€å¤§é¡µæ•°: {max_pages}")
    
    async with async_playwright() as p:
        for page_num in range(1, max_pages + 1):
            page_jobs = await fetch_page(page_num, p, target_date)
            
            if not page_jobs and page_num > 1:
                print(f"   ğŸ“„ ç¬¬ {page_num} é¡µæ— åŒ¹é…èŒä½ï¼Œåœæ­¢")
                break
            
            all_jobs.extend(page_jobs)
            print(f"      æ‰¾åˆ° {len(page_jobs)} æ¡åŒ¹é…èŒä½")
            
            if len(page_jobs) == 0 and page_num >= 3:
                print(f"   è¿ç»­æ— åŒ¹é…ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # ä¼‘æ¯ä¸€ä¸‹é¿å…è¿‡å¿«è¯·æ±‚
            await asyncio.sleep(1)
    
    # å»é‡
    seen = set()
    unique_jobs = []
    for job in all_jobs:
        if job["url"] not in seen:
            seen.add(job["url"])
            unique_jobs.append(job)
    
    print(f"âœ… å…±æ‰¾åˆ° {len(unique_jobs)} æ¡æ‹›è˜å…¬å‘Š")
    return unique_jobs


def main():
    parser = argparse.ArgumentParser(description="æŠ“å–å…¬è€ƒé›·è¾¾èŒä½åˆ—è¡¨")
    parser.add_argument("--date", help="ç›®æ ‡å¹´æœˆ YYYY-MM (å¦‚ 2025-12)", 
                        default=os.environ.get("COLLECT_DATE", ""))
    parser.add_argument("--pages", help="æœ€å¤§é¡µæ•°", type=int,
                        default=int(os.environ.get("MAX_PAGES", "5")))
    args = parser.parse_args()
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    target_date = args.date
    if target_date:
        print(f"ğŸ¯ ç›®æ ‡æ—¥æœŸ: {target_date}")
    else:
        print("ğŸ¯ ç›®æ ‡æ—¥æœŸ: æœ€æ–°èŒä½")
    
    jobs = asyncio.run(fetch_list(target_date, args.pages))
    
    if not jobs:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ‹›è˜å…¬å‘Š")
        return
    
    # ä¿å­˜
    date_str = target_date.replace("-", "") if target_date else datetime.now().strftime("%Y%m%d")
    output_file = DATA_DIR / f"job_list_{date_str}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š å…± {len(jobs)} æ¡èŒä½")
    for i, job in enumerate(jobs[:5], 1):
        print(f"   {i}. {job['title'][:50]}...")


if __name__ == "__main__":
    main()
