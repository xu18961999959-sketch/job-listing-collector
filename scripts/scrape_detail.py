#!/usr/bin/env python3
"""
æŠ“å–å•ä¸ªèŒä½è¯¦æƒ…

ä½¿ç”¨æ–¹æ³•:
    python scripts/scrape_detail.py --url "https://..."

è¾“å‡º: è¿½åŠ åˆ° data/temp_details.json
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

DATA_DIR = Path(__file__).parent.parent / "data"


async def fetch_detail(url: str) -> dict:
    """ä½¿ç”¨ Playwright æŠ“å–èŒä½è¯¦æƒ…"""
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ è¯·å®‰è£… playwright")
        return {"url": url, "content": "", "error": "playwright not installed"}
    
    result = {"url": url, "content": "", "title": ""}
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = await context.new_page()
        
        try:
            await page.goto(url, timeout=30000, wait_until="domcontentloaded")
            await asyncio.sleep(3)  # ç­‰å¾… JS æ¸²æŸ“
            
            # è·å–æ ‡é¢˜
            title = await page.title()
            result["title"] = title
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ­£æ–‡å†…å®¹
            content = await page.evaluate("""
                () => {
                    // å…¬è€ƒé›·è¾¾ç‰¹å®šé€‰æ‹©å™¨
                    const selectors = [
                        '.article-content',
                        '.detail-content', 
                        '.content-wrap',
                        '.post-content',
                        '.news-content',
                        '.main-content',
                        '#article-content',
                        '#content',
                        'article',
                        '.content',
                        '.main',
                        '[class*="content"]',
                        '[class*="article"]',
                        '[class*="detail"]'
                    ];
                    
                    for (const sel of selectors) {
                        try {
                            const el = document.querySelector(sel);
                            if (el && el.innerText && el.innerText.length > 100) {
                                return el.innerText.trim();
                            }
                        } catch(e) {}
                    }
                    
                    // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè·å– body å†…å®¹ä½†æ’é™¤å¯¼èˆªç­‰
                    const body = document.body.cloneNode(true);
                    const removeSelectors = ['nav', 'header', 'footer', '.nav', '.header', '.footer', '.sidebar', 'script', 'style'];
                    removeSelectors.forEach(sel => {
                        body.querySelectorAll(sel).forEach(el => el.remove());
                    });
                    
                    return body.innerText.trim();
                }
            """)
            
            result["content"] = content[:8000] if content else ""
            
            # é¢å¤–æå–æ—¥æœŸä¿¡æ¯
            date_text = await page.evaluate("""
                () => {
                    const dateSelectors = ['.date', '.time', '.publish-time', '.post-date', '[class*="date"]', '[class*="time"]'];
                    for (const sel of dateSelectors) {
                        try {
                            const el = document.querySelector(sel);
                            if (el) return el.innerText.trim();
                        } catch(e) {}
                    }
                    return '';
                }
            """)
            if date_text:
                result["date_text"] = date_text
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âš ï¸ æŠ“å–å¤±è´¥: {e}")
        finally:
            await browser.close()
    
    return result


def main():
    parser = argparse.ArgumentParser(description="æŠ“å–èŒä½è¯¦æƒ…")
    parser.add_argument("--url", required=True, help="èŒä½è¯¦æƒ…URL")
    args = parser.parse_args()
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ” æ­£åœ¨æŠ“å–: {args.url[:60]}...")
    result = asyncio.run(fetch_detail(args.url))
    
    # è¿½åŠ åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = DATA_DIR / "temp_details.json"
    
    details = []
    if temp_file.exists():
        with open(temp_file, "r", encoding="utf-8") as f:
            try:
                details = json.load(f)
            except:
                details = []
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing_urls = {d.get("url") for d in details}
    if args.url not in existing_urls:
        details.append(result)
    
    with open(temp_file, "w", encoding="utf-8") as f:
        json.dump(details, f, ensure_ascii=False, indent=2)
    
    if result.get("error"):
        print(f"âŒ å¤±è´¥: {result['error']}")
    else:
        content_len = len(result.get('content', ''))
        print(f"âœ… æˆåŠŸ: {result.get('title', 'N/A')[:40]}...")
        print(f"   å†…å®¹é•¿åº¦: {content_len} å­—ç¬¦")
        if content_len < 100:
            print(f"   âš ï¸ å†…å®¹å¯èƒ½æå–ä¸å®Œæ•´")


if __name__ == "__main__":
    main()
