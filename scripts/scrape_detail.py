#!/usr/bin/env python3
"""
æŠ“å–å•ä¸ªèŒä½è¯¦æƒ…

ä½¿ç”¨æ–¹æ³•:
    python scripts/scrape_detail.py --url "https://..."

è¾“å‡º: è¿½åŠ åˆ° data/temp_details.json
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

DATA_DIR = Path(__file__).parent.parent / "data"


async def fetch_detail(url: str) -> dict:
    """ä½¿ç”¨ Playwright æŠ“å–èŒä½è¯¦æƒ…"""
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ è¯·å®‰è£… playwright")
        return {"url": url, "content": "", "error": "playwright not installed"}
    
    result = {"url": url, "content": "", "title": ""}
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto(url, timeout=30000)
            await page.wait_for_load_state("networkidle", timeout=15000)
            
            # è·å–æ ‡é¢˜
            title = await page.title()
            result["title"] = title
            
            # è·å–æ­£æ–‡å†…å®¹
            content = await page.evaluate("""
                () => {
                    const selectors = [
                        '.article-content', '.content', '.post-content',
                        '.detail-content', '#content', 'article', '.main'
                    ];
                    for (const sel of selectors) {
                        const el = document.querySelector(sel);
                        if (el) {
                            return el.innerText;
                        }
                    }
                    return document.body.innerText;
                }
            """)
            
            result["content"] = content[:5000] if content else ""
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âš ï¸ æŠ“å–å¤±è´¥: {e}")
        finally:
            await browser.close()
    
    return result


def main():
    parser = argparse.ArgumentParser(description="æŠ“å–èŒä½è¯¦æƒ…")
    parser.add_argument("--url", required=True, help="èŒä½è¯¦æƒ…URL")
    args = parser.parse_args()
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ” æ­£åœ¨æŠ“å–: {args.url[:60]}...")
    result = asyncio.run(fetch_detail(args.url))
    
    # è¿½åŠ åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = DATA_DIR / "temp_details.json"
    
    details = []
    if temp_file.exists():
        with open(temp_file, "r", encoding="utf-8") as f:
            try:
                details = json.load(f)
            except:
                details = []
    
    details.append(result)
    
    with open(temp_file, "w", encoding="utf-8") as f:
        json.dump(details, f, ensure_ascii=False, indent=2)
    
    if result.get("error"):
        print(f"âŒ å¤±è´¥: {result['error']}")
    else:
        print(f"âœ… æˆåŠŸ: {result['title'][:50]}...")
        print(f"   å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")


if __name__ == "__main__":
    main()
